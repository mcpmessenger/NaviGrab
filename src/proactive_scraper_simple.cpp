#include <iostream>
#include <string>
#include <vector>
#include <map>
#include <fstream>
#include <filesystem>
#include <chrono>
#include <thread>
#include <queue>
#include <set>
#include <windows.h>
#include <gdiplus.h>
#include <sstream>
#include <iomanip>
#include <regex>

#pragma comment(lib, "gdiplus.lib")

using namespace Gdiplus;

// Simple data structures for our scraper
struct ScrapedPage {
    std::string url;
    std::string title;
    std::string base64_screenshot;
    std::vector<std::string> found_links;
    std::vector<std::string> clicked_elements;
    std::chrono::system_clock::time_point timestamp;
    int depth;
    std::string parent_url;
};

struct ScrapingSession {
    int session_id;
    std::string start_url;
    int max_depth;
    int max_pages;
    std::set<std::string> visited_urls;
    std::queue<std::pair<std::string, int>> url_queue; // url, depth
    std::vector<ScrapedPage> scraped_pages;
    std::string output_directory;
    bool is_running;
    int pages_scraped;
};

class SimpleProactiveScraper {
private:
    ULONG_PTR gdiplusToken;
    GdiplusStartupInput gdiplusStartupInput;
    std::map<int, ScrapingSession> sessions;
    int next_session_id;

public:
    SimpleProactiveScraper() : next_session_id(1) {
        GdiplusStartup(&gdiplusToken, &gdiplusStartupInput, NULL);
        
        // Create output directory
        std::filesystem::create_directories("./scraped_data");
        std::filesystem::create_directories("./scraped_data/screenshots");
        std::filesystem::create_directories("./scraped_data/thumbnails");
    }
    
    ~SimpleProactiveScraper() {
        GdiplusShutdown(gdiplusToken);
    }
    
    int StartScraping(const std::string& start_url, int max_depth = 3, int max_pages = 20) {
        ScrapingSession session;
        session.session_id = next_session_id++;
        session.start_url = start_url;
        session.max_depth = max_depth;
        session.max_pages = max_pages;
        session.output_directory = "./scraped_data";
        session.is_running = true;
        session.pages_scraped = 0;
        
        // Add start URL to queue
        session.url_queue.push({start_url, 0});
        session.visited_urls.insert(start_url);
        
        sessions[session.session_id] = session;
        
        // Start scraping in background thread
        std::thread scraping_thread(&SimpleProactiveScraper::ScrapingWorker, this, session.session_id);
        scraping_thread.detach();
        
        return session.session_id;
    }
    
    void StopScraping(int session_id) {
        if (sessions.find(session_id) != sessions.end()) {
            sessions[session_id].is_running = false;
        }
    }
    
    std::vector<ScrapedPage> GetResults(int session_id) {
        if (sessions.find(session_id) != sessions.end()) {
            return sessions[session_id].scraped_pages;
        }
        return {};
    }
    
    bool IsRunning(int session_id) {
        if (sessions.find(session_id) != sessions.end()) {
            return sessions[session_id].is_running;
        }
        return false;
    }
    
    int GetPagesScraped(int session_id) {
        if (sessions.find(session_id) != sessions.end()) {
            return sessions[session_id].pages_scraped;
        }
        return 0;
    }

private:
    void ScrapingWorker(int session_id) {
        auto& session = sessions[session_id];
        
        while (session.is_running && !session.url_queue.empty() && session.pages_scraped < session.max_pages) {
            auto [current_url, depth] = session.url_queue.front();
            session.url_queue.pop();
            
            if (depth > session.max_depth) {
                continue;
            }
            
            std::cout << "ðŸ” Scraping: " << current_url << " (depth: " << depth << ")" << std::endl;
            
            // Simulate page scraping (in real implementation, this would use a real browser)
            ScrapedPage page = ScrapePage(current_url, depth, session.session_id);
            session.scraped_pages.push_back(page);
            session.pages_scraped++;
            
            // Find and queue new links
            for (const auto& link : page.found_links) {
                if (session.visited_urls.find(link) == session.visited_urls.end()) {
                    session.visited_urls.insert(link);
                    session.url_queue.push({link, depth + 1});
                }
            }
            
            // Save page data
            SavePageData(page, session.session_id);
            
            // Small delay to prevent overwhelming
            std::this_thread::sleep_for(std::chrono::milliseconds(1000));
        }
        
        session.is_running = false;
        std::cout << "âœ… Scraping session " << session_id << " completed. Pages scraped: " << session.pages_scraped << std::endl;
    }
    
    ScrapedPage ScrapePage(const std::string& url, int depth, int session_id) {
        ScrapedPage page;
        page.url = url;
        page.title = "Page " + std::to_string(session_id) + "_" + std::to_string(depth);
        page.depth = depth;
        page.timestamp = std::chrono::system_clock::now();
        
        // Take screenshot
        page.base64_screenshot = CaptureScreenshot();
        
        // Simulate finding links (in real implementation, this would parse HTML)
        page.found_links = SimulateLinkDiscovery(url);
        
        // Simulate clicking elements
        page.clicked_elements = SimulateElementClicking();
        
        return page;
    }
    
    std::string CaptureScreenshot() {
        // Get screen dimensions
        int screenWidth = GetSystemMetrics(SM_CXSCREEN);
        int screenHeight = GetSystemMetrics(SM_CYSCREEN);
        
        // Create device context for screen
        HDC screenDC = GetDC(NULL);
        HDC memoryDC = CreateCompatibleDC(screenDC);
        
        // Create bitmap
        HBITMAP bitmap = CreateCompatibleBitmap(screenDC, screenWidth, screenHeight);
        HBITMAP oldBitmap = (HBITMAP)SelectObject(memoryDC, bitmap);
        
        // Copy screen to bitmap
        BitBlt(memoryDC, 0, 0, screenWidth, screenHeight, screenDC, 0, 0, SRCCOPY);
        
        // Create GDI+ bitmap from HBITMAP
        Bitmap* gdiBitmap = new Bitmap(bitmap, NULL);
        
        // Convert to base64
        std::string base64 = BitmapToBase64(gdiBitmap);
        
        // Cleanup
        delete gdiBitmap;
        SelectObject(memoryDC, oldBitmap);
        DeleteObject(bitmap);
        DeleteDC(memoryDC);
        ReleaseDC(NULL, screenDC);
        
        return base64;
    }
    
    std::string BitmapToBase64(Bitmap* bitmap) {
        // Create IStream for bitmap
        IStream* stream = nullptr;
        CreateStreamOnHGlobal(NULL, TRUE, &stream);
        
        // Save bitmap as PNG to stream
        CLSID pngClsid;
        GetEncoderClsid(L"image/png", &pngClsid);
        bitmap->Save(stream, &pngClsid, NULL);
        
        // Get stream size
        LARGE_INTEGER li;
        li.QuadPart = 0;
        stream->Seek(li, STREAM_SEEK_END, NULL);
        ULONG streamSize = (ULONG)li.QuadPart;
        stream->Seek(li, STREAM_SEEK_SET, NULL);
        
        // Read stream data
        std::vector<BYTE> buffer(streamSize);
        ULONG bytesRead;
        stream->Read(buffer.data(), streamSize, &bytesRead);
        
        // Convert to base64
        std::string base64 = EncodeBase64(buffer.data(), bytesRead);
        
        stream->Release();
        return base64;
    }
    
    std::string EncodeBase64(const BYTE* data, size_t length) {
        const char* chars = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
        std::string result;
        int val = 0, valb = -6;
        
        for (size_t i = 0; i < length; i++) {
            val = (val << 8) + data[i];
            valb += 8;
            while (valb >= 0) {
                result.push_back(chars[(val >> valb) & 0x3F]);
                valb -= 6;
            }
        }
        
        if (valb > -6) result.push_back(chars[((val << 8) >> (valb + 8)) & 0x3F]);
        while (result.size() % 4) result.push_back('=');
        
        return result;
    }
    
    int GetEncoderClsid(const WCHAR* format, CLSID* pClsid) {
        UINT num = 0;
        UINT size = 0;
        ImageCodecInfo* pImageCodecInfo = NULL;
        GetImageEncodersSize(&num, &size);
        if (size == 0) return -1;
        
        pImageCodecInfo = (ImageCodecInfo*)(malloc(size));
        if (pImageCodecInfo == NULL) return -1;
        
        GetImageEncoders(num, size, pImageCodecInfo);
        for (UINT j = 0; j < num; ++j) {
            if (wcscmp(pImageCodecInfo[j].MimeType, format) == 0) {
                *pClsid = pImageCodecInfo[j].Clsid;
                free(pImageCodecInfo);
                return j;
            }
        }
        free(pImageCodecInfo);
        return -1;
    }
    
    std::vector<std::string> SimulateLinkDiscovery(const std::string& url) {
        // Simulate finding links on the page
        std::vector<std::string> links;
        
        // Generate some mock links based on the URL
        std::string base_url = url;
        if (base_url.back() == '/') base_url.pop_back();
        
        links.push_back(base_url + "/about");
        links.push_back(base_url + "/contact");
        links.push_back(base_url + "/products");
        links.push_back(base_url + "/services");
        links.push_back(base_url + "/blog");
        
        return links;
    }
    
    std::vector<std::string> SimulateElementClicking() {
        // Simulate clicking various elements
        return {"button.submit", "a.nav-link", "div.clickable", "form input[type=submit]"};
    }
    
    void SavePageData(const ScrapedPage& page, int session_id) {
        // Save screenshot to file
        std::string screenshot_filename = "screenshot_" + std::to_string(session_id) + "_" + 
                                        std::to_string(page.depth) + "_" + 
                                        std::to_string(std::chrono::duration_cast<std::chrono::milliseconds>(page.timestamp.time_since_epoch()).count()) + ".png";
        
        std::ofstream screenshot_file("./scraped_data/screenshots/" + screenshot_filename, std::ios::binary);
        if (screenshot_file.is_open()) {
            // Decode base64 and save as PNG
            std::string decoded = DecodeBase64(page.base64_screenshot);
            screenshot_file.write(decoded.c_str(), decoded.length());
            screenshot_file.close();
        }
        
        // Save thumbnail (smaller version)
        std::string thumbnail_filename = "thumb_" + screenshot_filename;
        SaveThumbnail(page.base64_screenshot, "./scraped_data/thumbnails/" + thumbnail_filename);
        
        // Save metadata as JSON
        std::string metadata_filename = "metadata_" + std::to_string(session_id) + "_" + 
                                      std::to_string(page.depth) + ".json";
        
        std::ofstream metadata_file("./scraped_data/" + metadata_filename);
        if (metadata_file.is_open()) {
            metadata_file << "{\n";
            metadata_file << "  \"url\": \"" << page.url << "\",\n";
            metadata_file << "  \"title\": \"" << page.title << "\",\n";
            metadata_file << "  \"depth\": " << page.depth << ",\n";
            metadata_file << "  \"timestamp\": \"" << std::chrono::duration_cast<std::chrono::milliseconds>(page.timestamp.time_since_epoch()).count() << "\",\n";
            metadata_file << "  \"screenshot\": \"" << screenshot_filename << "\",\n";
            metadata_file << "  \"thumbnail\": \"" << thumbnail_filename << "\",\n";
            metadata_file << "  \"links_found\": " << page.found_links.size() << ",\n";
            metadata_file << "  \"elements_clicked\": " << page.clicked_elements.size() << "\n";
            metadata_file << "}\n";
            metadata_file.close();
        }
    }
    
    std::string DecodeBase64(const std::string& encoded) {
        const std::string chars = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
        std::string result;
        int val = 0, valb = -8;
        
        for (char c : encoded) {
            if (chars.find(c) == std::string::npos) break;
            val = (val << 6) + chars.find(c);
            valb += 6;
            if (valb >= 0) {
                result.push_back(char((val >> valb) & 0xFF));
                valb -= 8;
            }
        }
        
        return result;
    }
    
    void SaveThumbnail(const std::string& base64_data, const std::string& filename) {
        // For now, just copy the full image as thumbnail
        // In a real implementation, this would resize the image
        std::string decoded = DecodeBase64(base64_data);
        std::ofstream thumbnail_file(filename, std::ios::binary);
        if (thumbnail_file.is_open()) {
            thumbnail_file.write(decoded.c_str(), decoded.length());
            thumbnail_file.close();
        }
    }
};

// C API for easy integration
extern "C" {
    SimpleProactiveScraper* create_scraper() {
        return new SimpleProactiveScraper();
    }
    
    void destroy_scraper(SimpleProactiveScraper* scraper) {
        delete scraper;
    }
    
    int start_scraping(SimpleProactiveScraper* scraper, const char* url, int max_depth, int max_pages) {
        if (!scraper) return -1;
        return scraper->StartScraping(std::string(url), max_depth, max_pages);
    }
    
    void stop_scraping(SimpleProactiveScraper* scraper, int session_id) {
        if (scraper) {
            scraper->StopScraping(session_id);
        }
    }
    
    int is_scraping_running(SimpleProactiveScraper* scraper, int session_id) {
        if (!scraper) return 0;
        return scraper->IsRunning(session_id) ? 1 : 0;
    }
    
    int get_pages_scraped(SimpleProactiveScraper* scraper, int session_id) {
        if (!scraper) return 0;
        return scraper->GetPagesScraped(session_id);
    }
}


